\input{style}

\title{Inference of Bifurcations\\with Differentiable Continuation}
\author{
        Gregory Szep,\quad Attila Csik\'asz-Nagy\\King's College London\\London, WC2R 2LS\\
        \texttt{gregory.szep@kcl.ac.uk}
    \And
        Neil Dalchau\\Microsoft Research Cambridge\\Cambridge, CB1 2FB\\
        \texttt{ndalchau@microsoft.com}
}

\begin{document}
\input{notation}
\maketitle

\begin{abstract}
    In this work we propose a gradient-based semi-supervised approach for matching target bifurcations with parameterised differential equation models. The cost function contains a supervised term that is minimal when predicted bifurcations match the targets and an unsupervised bias that encourages bifurcations by maximising zero crossings in the determinant of the Jacobian. The calculation of gradients with respect to parameters shares the same computational complexity as deflated pseudo-arclength continuation used to calculate the bifurcation diagram. We demonstrate model synthesis with minimal models which explore the space of saddle-node and pitchfork diagrams, a genetic toggle switch from synthetic biology and the FitzHugh-Nagumo model. Furthermore, the cost landscape allows us to  organise models in terms of topological and geometric equivalence.
\end{abstract}

\section{Introduction}

Backpropagation through differential equation solves has been a breakthrough over the past couple of years \cite{Chen2018NeuralEquations,Rackauckas2019DiffEqFlux.jl-AEquations} that enabled scalable parameter inference for differential equations. Determining what model parameters should be for a given set of observations, in the setting of biology and engineering, are known as inverse problems \cite{Abdulla2009InverseBiology}.

Optimisation targets, however, have mostly been expressed in the spatio-temporal domain. Acquisition of such data can be costly and can often contain over-constraining information generated by processing steps or the measurement device rather than the state variables that underpin the observed mechanism. In microscopy, for example, data is often reported in arbitrary fluoresce units allowing the observer to shift and scale data arbitrarily. Furthermore, such data may also not contain sufficient information about dynamical transients in order to identify kinetic parameters. The emerging picture suggests that identification of the qualitative behaviour -- the bifurcation diagram -- should precede any attempt at inferring kinetic parameters \cite{Stumpf2019ParameterBifurcations}. Techniques for back-propagating through implicit equation solvers have also been developed \cite{Look2020DifferentiableLayers,Bai2019DeepModels} although to the best of the authors' knowledge have not been applied to bifurcation diagrams at the time of writing this paper.

In this work we propose a gradient-based semi-supervised approach that focuses on fitting high-level qualitative constraints, defined by state space structures, rather than kinetics. Drawing inspiration from implicit solvers \cite{Look2020DifferentiableLayers,Bai2019DeepModels} to calculate gradients we find that their computation shares the same  complexity as the algorithm used to calculate the bifurcation diagram. We use a predictor-corrector method called deflated pseudo-arclength continuation \cite{Farrell2016TheDiagrams,Veltz2019PseudoArcLengthContinuation.jl}, originally developed for partial differential equations. In the case of partial differential equations the computational complexity of calculating a single bifurcation diagram is not bounded since superpositions of localised solutions may give rise to uncountably many branches \cite{Avitabile2010ToEquation}. The complexity of computing a single branch, however, is bounded by the complexities of the chosen eigenvalue solver and corrector, and further decreased by adaptive stepping procedures \cite{Aruliah2016AlgorithmContinuation}.

We find that the cost function landscape contains basins that not only allow us to synthesise models with a desired bifurcation structure but also allow us to organise models in terms of topological and geometric equivalence. We discuss the relevance of this in model selection. In summary, our paper has the following main contributions:

\begin{itemize}
    \item Defined a differentiable semi-supervised cost function for encouraging and placing co-dimension one bifurcations to user-specified target locations
    \item Implementation of method in Julia package \texttt{FluxContinuation.jl} leveraging automatic differentiation tooling in \texttt{Flux.jl}
    \item Leveraging the cost landscape for a novel way of organising differential equation models in terms of geometric and topological equivalence
\end{itemize}

\subsection{Related Works | \textit{in progress}}

\paragraph{Bifurcation control} and \textit{inverse} bifurcation analysis literature applies control theory to enable design and identification of dynamical systems with qualitative constraints. The first works involved the calculation of the minimal distances to bifurcation manifolds \cite{Iwasaki1997AnType,Lu2006InverseSystems,Dobson2004DistanceBifurcations}. Those optimisations did not involve closed form expressions and the challenge of finding the manifolds to being with was left to random sampling.

\paragraph{Matching vector fields} From a dynamical systems perspective a given bifurcation curve constrains a state-vector field geometry. A set of target bifurcations may also be represented as a vector field flow and thus inferring bifurcations becomes a problem in matching flows in vector sub-spaces. Note that this would involve matching direction but not the magnitude of the flow.

\begin{itemize}
    \item smooth and match estimator methods \cite{Ranciati2017BayesianParameters}
    \item robots learning limit cycles \cite{Khadivar2021LearningBifurcations}
\end{itemize}

\paragraph{Non-differentiable methods} 
A large body of work is dedicated to finding conditions for fold-bifurcations, multistability and other

\begin{itemize}
    \item bifurcation inference using Mixed Integer Nonlinear Programming \cite{Otero-Muras2018Optimization-basedModels}
    \item Random sampling and genetic algorithms \cite{Chickarmane2005BifurcationTool,Conrad2006BifurcationClock}
\end{itemize}

\subsection{Preliminaries}
Suppose we parameterise a set of differential equations for states $u\in\Reals^N$ with a function $\rates$ in an unknown parameter space $\theta\in\Reals^M$. We would like these differential equations to obey a set of target bifurcations $\targets:=\{p_1\dots p_K\}$ along a known bifurcation parameter $p\in\Reals$. Let the differential equations be defined as
\begin{align}
	\partial_t u=\rates(z)
	\qquad\mathrm{where}\quad z:=(u,p)\quad
	\rates : \Reals^{N+1}\rightarrow\Reals^N
	\label{eq:model}
\end{align}
For a given set of parameters $\theta$ one could compute the set of predicted bifurcations $\predictions(\theta)$ using parameter continuation methods \cite{Veltz2019PseudoArcLengthContinuation.jl,Farrell2016TheDiagrams}. Our goal is to find optimal parameters $\theta^*$ that match predictions $\predictions(\theta^*)$ to specified targets $\targets$. We must design a suitable cost function $\loss$ so that
\begin{align}
    \theta^* := \mathrm{argmin}_{\theta} \loss(\theta|\targets)
    \label{eq:optimal-theta}
\end{align}
The optimal $\theta^*$ is not expected to always be unique, but is in general a manifold representing the space of qualitatively equivalent models. The cost function $\loss$ must have some distance measure between the two sets $\predictions(\theta)$ and $\targets$ with a bias that encourages equivalent cardinality $|\predictions(\theta)|=|\targets|$. This is especially important in the case where there are no predictions $|\predictions(\theta)|=0$.

For clarity, we guide the reader through the methods with the following minimal models that explore the space of saddle-nodes $\rates(z) = p + \theta_{1}u+\theta_{2}u^3$ and pitchforks $\rates(z) = \theta_{1} + p u+\theta_{2}u^3$. These minimal models are shown with targets $\targets$ in Figure \ref{fig:saddle-node} and Figure \ref{fig:pitchfork} respectively. These figures show that the determinant $\Det=0$ at the bifurcations and that its directional derivative along the bifurcation curve $\frac{d}{ds}\Det$ is finite and non-zero, at least in the non-degenerate case. This gives us our first hint of what should be optimised to increase the number of predictions $|\predictions(\theta)|$.

\begin{figure}
\centering
\includegraphics[width=11cm]{saddle-node}
\caption{Saddle-node model $\rates(z) = p + \theta_{1}u+\theta_{2}u^3$ with $\theta=(5/2,-1)$ and targets $\targets=\{-1,1\}$. Determinant in red. Lighter shades indicate the determinant crossing zero for unstable solutions}
\label{fig:saddle-node}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=11cm]{pitchfork}
\caption{Pitchfork model $\rates(z) = \theta_{1} + p u+\theta_{2}u^3$ with $\theta=(1/2,-1)$ and target $\targets=\{0\}$. Determinant in red. Lighter shades indicate the determinant crossing zero for unstable solutions}
\label{fig:pitchfork}
\end{figure}

\section{Proposed Method}
\subsection{Semi-supervised Cost Function}

\begin{figure}
    \centering
    \includegraphics[width=10cm]{bifurcation-measure}
    \caption{Measure $\measure(z)\rightarrow 1$ as bifurcations form where $\Det\rightarrow0$ for two different $\theta$}
    \label{fig:bifurcation-measure}
\end{figure}

In order for predicted bifurcations $p\in\predictions(\theta)$ to match targets $p'\in\targets$ we need to evaluate some form of error term $|p-p'|$ on the bifurcation curve defined by $\rates(z)=0$. We expect the supervised term to be some mean over the targets $\targets$ and predictions $\predictions(\theta)$. We choose a geometric mean over the predictions and an arithmetic mean over targets to account for cases where $|\predictions|\neq|\targets|$ and discourage multiple predictions matching the same target in cases where $|\predictions|>|\targets|$
\begin{align}
    \frac{1}{|\targets|}\sum_{p'\in\targets}
    \left(\prod_{p\in\predictions(\theta)}|p-p'|\right)
    ^{\frac{1}{|\predictions|+1}}
\end{align}
We can see from Figures \ref{fig:saddle-node} and \ref{fig:pitchfork} that predictions $\predictions(\theta)$ can be identified by looking for points $p$ along the curve where the determinant $\Det=0$. Meanwhile the directional derivative along the bifurcation curve is finite $\frac{d}{ds}\Det\neq 0$ in the non-degenerate case. Using these quantities we can define a positive semi-definite measure $\measure(z)$ of zero crossings in the determinant along the bifurcation curve; as shown in Figure \ref{fig:bifurcation-measure} this can be used as a proxy for the probability of a bifurcation at location $p$ for a given set of parameters $\theta$. The total bifurcation measure $\Psi(\theta)$ is written
\begin{align}
    \Psi(\theta):=\frac{
        \int_{\rates(z)=0}\!
        \measure(z)\,\mathrm{d}z
    }{
        \int_{\rates(z)=0}\!\
        \mathrm{d}z
    }
    \quad\mathrm{where}\quad
    \measure(z):=
    \left(1+\left|\frac{\Det}{\frac{d}{ds}\Det}\right|\right)^{-1}
    \label{eq:measure}
\end{align}
Here we denote $\int_{\rates(z)=0}\mathrm{d}z$ as the sum of the line integrals in $z\in\Reals^{N+1}$ defined by the steady states $\rates(z)=0$. The measure $\measure(z)=1$ at bifurcation points and goes to zero an odd number of times between bifurcations. This is because $\Det$ must eventually turn around in order to return back to zero, resulting in $\frac{d}{ds}\Det\rightarrow0$ and hence $\measure(z)\rightarrow0$ for each turning point. As $\Det$ diverges we approach regimes far away from any closest bifurcation and hence $\measure(z)\rightarrow0$. We would still like to have non-zero gradients with respect to $\theta$ in this regime, and therefore $\measure(z)$ was designed to go to zero slowly. The total measure $\Psi(\theta)$ is normalised such that $\Psi(\theta)\rightarrow1$ in the regimes where the observed parameter region $p$ is densely packed with bifurcations. In section \ref{section:measure} we will see that $\measure(z)$ is also invariant under certain classes of transformations on $\rates(z)$ which do not change bifurcation locations. With this measure in hand, the supervised term can be re-written as an explicit integration over the bifurcation curve
\begin{align}
    \frac{1}{|\targets|}\sum_{p'\in\targets}
    \exp\left[
        \frac{1}{|\predictions|+1}
            \int_{\rates(z)=0}\!\!\!\!\!\!\!\!\!\!\!\!\!\quad\measure(z)\,\,\log|p-p'|
        \,\mathrm{d}z
    \right]
    \label{eq:supervised-term}
\end{align}
The supervised term is equal to one when there are no predictions $|\predictions|=0$; the exponent is zero because the constraint $\Det=0$ along the curve $\rates(z)=0$ is never satisfied in the integration. It is equal to zero when $|\predictions|\geq|\targets|$ and all targets are matched by at least one prediction. Note that we dropped the explicit dependency on $\theta$ in counting the predictions $|\predictions|$. While the dependence still exists, gradients of this term with respect to $\theta$ are zero everywhere, and not defined when the number of predictions changes; the smooth dependency on $\theta$ is now picked up by the constraint $\Det=0$.

This curvature can be maximised to encourage bifurcations and should be used in the unsupervised term to allow an optimiser to follow non-zero gradients in parameter regimes $\theta$ far away from a bifurcating regime $|\predictions|=0$. The number of bifurcations must be reduced however, in cases where $|\predictions|>|\targets|$. A term of the following form emerges
\begin{align}
    \big(|\targets|-|\predictions|\big)f \circ K(\theta)
    \quad\mathrm{where}\quad f(x)> 0 \quad f(-x)=f(x)\quad\lim_{x\rightarrow\infty}f(x)=0
\end{align}
In makes sense compose the curvature $K$ with a positive definite symmetric function $f$ that goes to zero as the curvature diverges. The pre-factor ensures that the gradients are always pushing optimisers towards a state where $|\targets|=|\predictions|$. The remaining questions are: what should $f(0)$ be? How fast do we want the function to decay? These are implementation details that are discussed in section \ref{section:implementation}. In order to compute the curvature a differentiable representation of the bifurcation curve is needed. The following sections \ref{section:tangent-fields} and \ref{section:measure} outline how this can be done.

\subsection{Bifurcation Curves as Tangent Fields}
\label{section:tangent-fields}

Let each component of the vector function $\rates$ in the model \eqref{eq:model} implicitly define a surface embedded in $\Reals^{N+1}$. Let's assume that the intersection of these $N$ surfaces exists and is not null or degenerate, then the steady states of \eqref{eq:model} must be a set of one dimensional space curves in $z\in\Reals^{N+1}$ defined by
\begin{align}
    \rates(z) = 0
\end{align}
An expression for the field $\tangent(z)$ tangent to the set of curves would allow us to take derivatives and integrals along the bifurcation curve. This is exactly what we need to do to evaluate our cost function \ref{eq:cost}. Fortunately the tangent field can be constructed by ensuring it is perpendicular to the gradient $\partial_z$ of each component of $\rates$ as illustrated by an example two component system in Figure \ref{fig:implicit-surfaces}. The tangent field $\tangent(z)$ can be constructed perpendicular to all gradient vectors using the properties of the determinant \cite{Goldman2005CurvatureSurfaces}
\begin{align}
    \tangent(z):=
    \label{eq:tangent-field}
    \left|\begin{matrix}
        \hat{z} \\
        \,\partial_{z}\rates\,
    \end{matrix}\right|
    \qquad\tangent : \Reals^{N+1}\rightarrow\Reals^{N+1}\\
    =\sum_{i=1}^{N+1}\hat{z}_{i}(-1)^{i+1} \left|\frac{\partial \rates}{\partial(z\setminus z_{i}) }\right|
\end{align}
where $\hat{z}$ is a collection of unit basis vectors in the $\Reals^{N+1}$ space and $\partial_{z}\rates$ is an $N\times(N+1)$ rectangular Jacobian matrix of partial derivatives and $z\setminus z_{i}$ denotes the $N$ dimensional vector $z$ with component $z_{i}$ removed. This construction ensures perpendicularity to any gradients of $\rates$
\begin{align}
    \tangent(z)\cdot\partial_z f_{\theta} =
    \left|\begin{matrix}
        \partial_z f_{\theta} \\
        \,\partial_{z}\rates\,
    \end{matrix}\right|
    \quad =0 \quad\forall f_{\theta}\in \rates
\end{align}
since the determinant of any matrix with two identical rows or columns is zero. Note that the tangent field $\tangent(z)$ is actually defined for all values of $z$ where adjacent field lines trace out other level sets where $\rates(z)\neq0$. Furthermore deformations with respect to $\theta$ are always orthogonal to the tangent
\begin{align} % \todo{numerically true. analytic proof?}
    \tangent(z)\cdot\frac{d\tangent}{d\theta}=0
\end{align}
\begin{figure}
\centering
\includegraphics[width=13cm]{determinant-field}
\caption{Left/Right : Determinant $\Det$ and tangent field $\tangent(z)$ for the saddle-node/pitchfork models for some set values of $\theta$ revealing that $\Det=0$ defines bifurcations}
\label{fig:determinant-field}
\end{figure}

Figure \ref{fig:determinant-field} shows how the bifurcation curve defined by $\rates(z)=0$ picks out one of many level sets or traces in tangent field $\tangent(z)$ for the saddle and pitchfork. The tangent field $\tangent(z)$ can always be analytically evaluated by taking the determinant in \eqref{eq:tangent-field}. We will proceed with calculations on $\tangent(z)$ in the whole space $z$ and pick out a single trace by solving $\rates(z)=0$ later. For our two models
\begin{align}
    \underset{\mathrm{saddle-node\,\,model}}{
    \tangent(z)=\hat{u}-(\,3\theta_2 u^2+\theta_1\,)\,\hat{p}}
    \qquad\qquad
    \underset{\mathrm{pitchfork\,\,model}}{
    \tangent(z)=u\hat{u}-(\,3\theta_2 u^2+p\,)\,\hat{p}}
    \label{eq:tangent-field-examples}
\end{align}

\subsection{Evaluating the Measure}
\label{section:measure}

Figure \ref{fig:determinant-field} reveals that $\Det=0$ is also a level set and that the intersection with level set $\rates(z)=0$ defines the bifurcations at specific parameter $\theta$. In this particular setting we can see that the tangent field $\tangent(z)$ only folds when $\Det=0$. Plotting the value of the determinant along $\rates(z)=0$ from Figure \ref{fig:determinant-field} would give rise to Figures \ref{fig:saddle-node} and \ref{fig:pitchfork}.

The directional derivative of the determinant $\Det$ along the tangent field $\tangent(z)$ is defined as
\begin{align}
    \frac{d}{ds}\Det := \hat{\tangent}(z) \cdot \frac{\partial}{\partial z}\Det 
\end{align}
where $\hat{\tangent}(z)$ is the unit tangent field


% While the unsupervised integrand has an smooth analytic expression, such as those evaluated in \eqref{eq:curvature-examples}, the constraint $\Det=0$ in the first integral makes differentiation with respect to $\theta$ more tricky. Here it becomes convenient to re-express the constraint as a delta function $\delta(z)$ and applying Lebesgue's dominated convergence theorem yielding
% \begin{align}
%     \lim_{n\rightarrow\infty}\int_{\steadystates}
%     \delta_{n}\left(\Det\right)
%     \frac{1}{|\targets|}\sum_{p'\in\targets} |p-p'|^2\, \mathrm{d}u\mathrm{d}p
% \end{align}
% where $\delta_{n}(z)$ can be any basis function whos limit as $n\rightarrow\infty$ is a delta function $\delta(z)$. In practice one could choose the normal distribution function with sufficiently small variance.

% The cost function is thus evaluated over the implicitly specified region $\steadystates$ --- in most cases the region will require a numerical procedure to find the roots $\rates(z)=0$ in the region $p\in\Omega$. In a similar vein to back-propagation through neural differential equations \cite{Chen2018NeuralEquations} we would like to be able to differentiate the cost $\loss(\theta|\targets,\Omega)$ without having to differentiate through the operations of the solver that finds $\rates(z)=0$. Fortunately this is all possible if we apply a similar strategy to that used by implicit layers \cite{Look2020DifferentiableLayers}; see Appendix \ref{appendix:space-curve} and \ref{appendix:deformation} for derivations.

\subsection{Implementation}
\label{section:implementation}
%/ move to appendix?
Here provide a high-level view of Algorithm \ref{alg:optimisation-loop}. We use \texttt{BifurcationKit.jl} \cite{Veltz2019PseudoArcLengthContinuation.jl} to calculate bifurcation diagrams, which requires the rate function $\rates$, its Jacobian $J_\theta$ which can be calculated using \texttt{ForwardDiff.jl} \cite{Revels2016Forward-ModeJulia}, and an initial guess $u_0$ at an initial parameter $p_0$. Let this algorithm be called by function \texttt{getSteadyStates} and return the steady states that form the integration region $\steadystates$ and a new guess $u_0$ that is the true steady state at $p_0$.

The algorithm also takes hyperparameters $\beta$ which will contain arclength step sizes, eigenvalue and Newton solver options and unsupervised correction factor $\lambda$. The adaptive hyperparameter update is done by \texttt{updateHyperparameters} and should depend on the current solutions $\steadystates$ and the targets $\targets$. The step sizes and solver options should adapt to the expected space of the solutions $\steadystates$ so that the waiting time between iterations is minimised. The unsupervised correction factor $\lambda$ should be large when there are no predicted bifurcations and decrease as the number of predicted bifurcations approaches the number of targets.

The evaluation of the cost gradient is done by \texttt{costGradient} using \texttt{ForwardDiff.jl} \cite{Revels2016Forward-ModeJulia} which has a large repository of chain rules that it uses to correctly evaluate derivatives. At the time of writing this paper the repository does not include the Leibniz rule \cite{Flanders1973DifferentiationSign} for implicitly defined line integrals, and we therefore define additional rules that implement the results shown in Appendix \ref{appendix:space-curve} and \ref{appendix:deformation}. The cost gradient is used by \texttt{updateParameters}, which can be any gradient-based optimiser such as ADAM or Momentum gradient decent, to update the parameters $\theta$.

In practice the bifurcation curve is only evaluated in a finite region $p\in\Omega$ which includes the targets $\targets$. We are now ready to write down the semi-supervised cost function for a bifurcation curve defined by $\rates(z)=0$ in an observed region $p\in\Omega$.

\begin{algorithm*}[H]
\label{alg:optimisation-loop}
\SetAlgoLined
\textbf{Inputs} Function $F_{\theta}:\mathbb{R}^{N+1}\rightarrow\mathbb{R}^{N}$ for $u\in\mathbb{R}^N$ and $p\in\mathbb{R}$\\ target bifurcations $\mathcal{D}=\{p_1\dots p_K:p\in\Omega\}$ and convergence tolerance $\varepsilon$\\
\textbf{Output} Optimised $\theta\in\mathbb{R}^M$ that satisfy targets $\targets$\\
$u_0\leftarrow\mathrm{rand}\in\mathbb{R}^{N}$\quad\,$\theta\leftarrow\mathrm{rand}\in\mathbb{R}^{M}$\\
$\beta\leftarrow\texttt{getHyperparameters}(\targets)$\\
\While{$\mathrm{tolerance}(\beta)>\varepsilon$}{
  $\steadystates,u_0\leftarrow\texttt{getSteadyStates}(F_{\theta},u_0,\beta)$\\
  $\beta\leftarrow\texttt{updateHyperparameters}(\beta,\steadystates,\targets)$\\
  $\partial L\leftarrow\texttt{costGradient}(F_{\theta},\steadystates,\targets,\beta)$\\
  $\theta\leftarrow\texttt{updateParameters}(\theta,\partial L,\beta)$
}
\caption{Bifurcation Optimisation Loop}
\end{algorithm*}

\section{Experiments \& Results}

\subsection{Minimal Models}
Figures \ref{fig:saddle-node:results} and \ref{fig:pitchfork:results} show example optimisations of $(\theta_1,\theta_2)$ for the minimal saddle-node and pitchfork models respectively. The magnitude of the correction factor $\lambda=0$ when bifurcations are present and $\lambda\neq0$ otherwise. Optimisation trajectories approach lines of global minima in the cost function, which represent a set of geometrically equivalent models, depicted in the right panels. Two bifurcation curves are geometrically equivalent if the number, type and locations of bifurcations match.

We can see that the geometrically equivalent lines are contained within larger basins defined by $\lambda=0$ where the correct number and type of bifurcations are present $p\in\Omega$, but do not match the locations of targets $\targets$. All models within this basin are in some sense topologically equivalent. This hierarchical classification allows us to identify the set of models that satisfy observed qualitative behaviour \cite{Stumpf2019ParameterBifurcations} before any attempt at inferring kinetic parameters, which is done by choosing a model along the line of geometrically equivalent models.

Optimisation trajectories for the two minimal models appear mostly circumferential. This is because the models were set up such that the radial direction from the origin in $\theta$ space mostly scale kinetics whereas the circumferential direction changes the bifurcation topology. This suggests that the gradients of our cost function seek to change model geometry over kinetics.

\begin{figure}
\centering
\includegraphics[width=6cm]{saddle-landscape.png}
\includegraphics[width=6cm]{saddle-optima.png}
\caption{Saddle-node model $\rates(u,p) = p + \theta_{1}u+\theta_{2}u^3$ optimised with respect to targets $\targets=\{-1,1\}$ in observation region $\Omega\in[-2,2]$. The right panel shows bifurcations diagrams for the three optima marked by stars on the left panel. The optimisation trajectories in white follow the gradient of the cost, approaching the black line of global minima in the left panel}
\label{fig:saddle-node:results}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=6cm]{pitchfork-landscape.png}
\includegraphics[width=6cm]{pitchfork-optima.png}
\caption{Pitchfork model $\rates(u,p) = \theta_{1} + u p +\theta_{2}u^3$ optimised with respect to targets $\targets=\{0\}$ in observation region $\Omega\in[-5,5]$. The right panel shows bifurcations diagrams for the three optima marked by stars on the left panel. The optimisation trajectories in white follow the gradient of the cost, approaching the black lines of global minima in the left panel}
\label{fig:pitchfork:results}
\end{figure}

\subsection{Genetic Toggle Switch}
In this section we optimise a more complicated model, popular in the synthetic biology community
\begin{equation}
    %\dfrac{du_1}{dt} = \dfrac{a_1} { 1 + (p u_2)^2 } - \mu_1 u_1, \quad
    %\dfrac{du_2}{dt} = \dfrac{a_2} { 1 + (k u_1)^2 } - \mu_2 u_2
    \dfrac{du_1}{dt} = \dfrac{a_1 + (p u_2)^2} { 1 + (p u_2)^2 } - \mu_1 u_1, \quad
    \dfrac{du_2}{dt} = \dfrac{a_2 + (k u_1)^2} { 1 + (k u_1)^2 } - \mu_2 u_2
\end{equation}


\subsection{Hyperparameters}


% \subsubsection{Genetic toggle switch}
% \subsubsection{FitzHugh-Nagumo}



% Without loss of generality, we can rescale $u_1$ by $a_1$ and $u_2$ by $a_2$:
% \begin{equation*}
%     \dfrac{du_1}{dt} = \dfrac{1} { 1 + (\hat{p} u_2)^2 } - \mu_1 u_1, \quad
%     \dfrac{du_2}{dt} = \dfrac{1} { 1 + (\hat{k} u_1)^2 } - \mu_2 u_2
% \end{equation*}
% where $\hat{p} = p a_2$ and $\hat{k} = k a_1$. The equilibria for this system are defined by
% \begin{equation}
%     \mu_1u_1^*(1+\hat{p}u_2^*)^2 = 1, \quad \mu_2u_2^*(1+\hat{k}u_1^*)^2 = 1
% \end{equation}
% Obtaining closed form expressions for $u_1^*$ and $u_2^*$ requires solving a quintic polynomial. We won't do that here.

% What is the determinant for this system?
% \begin{equation*}
%     \left|\dfrac{\partial F}{\partial u}\right| = 
%     \begin{vmatrix} -\mu_1 & -\frac{2 p^2 u_2^*}{(1 + (p u_2^*)^2)^2} \\
%         -\frac{2k^2u_1^*}{(1+(k u_1^*)^2)^2} & -\mu_2
%     \end{vmatrix}
%     = \mu_1\mu_2 - \frac{4p^2k^2u_1^*u_2^*}{(1+(p u_2^*)^2)^2(1+(k u_1^*)^2)^2}
% \end{equation*}

% \subsubsection{Two-state (generalised)}
% \begin{equation}
%     \dfrac{du_1}{dt} = \dfrac{a_1 + b_1(p u_2)^2} { 1 + (p u_2)^2 } - \mu_1 u_1, \quad
%     \dfrac{du_2}{dt} = \dfrac{a_2 + b_2(k u_1)^2} { 1 + (k u_1)^2 } - \mu_2 u_2
% \end{equation}

% Applying the same rescaling as above, we can simplify as
% \begin{equation}
%     \dfrac{du_1}{dt} = \dfrac{1 + \rho_1(p u_2)^2} { 1 + (p u_2)^2 } - \mu_1 u_1, \quad
%     \dfrac{du_2}{dt} = \dfrac{1 + \rho_2(k u_1)^2} { 1 + (k u_1)^2 } - \mu_2 u_2
% \end{equation}
% where $\rho_i = \frac{b_i}{a_i}$, with other parameters rescaled as above (but dropping hats).

\subsection{Computational Complexity}

\lipsum[1]

\section{Conclusion \& Broader Impact}

\lipsum[1]

% \section*{Checklist}
% \input{checklist}

\bibliography{refs}
\bibliographystyle{ieeetr}

\section*{Appendix}
\appendix
\input{appendix}

\end{document}
