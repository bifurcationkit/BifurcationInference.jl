\documentclass{article}
\PassOptionsToPackage{numbers,compress}{natbib}
\usepackage{lipsum}

% \usepackage{neurips_2021}
% \usepackage[preprint]{neurips_2021}
\usepackage[final]{neurips_2021}

\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage{url}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=blue,
    citecolor=blue,      
    urlcolor=cyan,
}

\usepackage{graphicx,xcolor,float}
\graphicspath{{./figures/}{docs/figures/}}

\usepackage{booktabs,amsmath,amsfonts}
\usepackage{nicefrac,microtype}

\usepackage{caption,subcaption}
\renewcommand\thesubfigure{\Alph{subfigure}}

\usepackage[ruled,vlined]{algorithm2e}
\usepackage{wrapfig}

\title{Parameter Inference\\with Bifurcation Diagrams}
\author{
        Gregory Szep\\King's College London\\London, WC2R 2LS, UK\\
        \texttt{gregory.szep@kcl.ac.uk}
    \And
        Attila Csik\'asz-Nagy\\P\'azm\'any P\'eter Catholic University\\Budapest, 1083, Hungary\\
        \texttt{csikasznagy@gmail.com}
    \And
        Neil Dalchau\\Microsoft Research Cambridge\\Cambridge, CB1 2FB, UK\\
        \texttt{ndalchau@gmail.com}
}

\begin{document}

\newcommand{\rates}{F_{\theta}}
\newcommand{\tangent}{T_{\theta}}
\newcommand{\steadystates}{\partial S_{\theta}}

\newcommand{\Det}{\left| \frac{\partial\rates}{\partial u} \right|}
\newcommand{\measure}{\varphi_{\theta}}

\newcommand{\predictions}{\mathcal{P}}
\newcommand{\targets}{\mathcal{D}}
\newcommand{\loss}{L}
\newcommand{\error}{E}
\newcommand{\Reals}{\mathbb{R}}

\maketitle
\begin{abstract}
    Estimation of parameters in differential equation models can be achieved by applying learning algorithms to quantitative time-series data. However, sometimes it is only possible to measure qualitative changes of a system in response to a controlled condition. In dynamical systems theory, such change points are known as \textit{bifurcations} and lie on a function of the controlled condition called the \textit{bifurcation diagram}. In this work, we propose a gradient-based approach for inferring the parameters of differential equations that produce a user-specified bifurcation diagram. The cost function contains an error term that is minimal when the model bifurcations match the specified targets and a bifurcation measure which has gradients that push optimisers towards bifurcating parameter regimes. The gradients can be computed without the need to differentiate through the operations of the solver that was used to compute the diagram. We demonstrate parameter inference with minimal models which explore the space of saddle-node and pitchfork diagrams and the genetic toggle switch from synthetic biology. Furthermore, the cost landscape allows us to organise models in terms of topological and geometric equivalence.
\end{abstract}

\section{Introduction}
\label{section:introduction}

Inverse problems \cite{Abdulla2009InverseBiology} arise in biology and engineering in settings when the model is not fully known and the desire is to match model behaviour to a given set of observations. This helps systematically guide both model and experimental design. While we would like to understand the quantitative details of a system, often only qualitative changes in response to varying experimental conditions can be robustly measured across independent studies \cite{Tyson2001NetworkPhysiology,Grant2020InterpretationCircuit}. For example, several studies are likely to agree that the human immune system activates above a threshold concentration of a pathogen and deactivates at a lower threshold concentration, but may disagree on the exact quantities of the thresholds or the magnitudes of the immune response. Bifurcation theory provides us a framework for studying these transitions in a manner that is independent of quantitative details \cite{Kuznetsov2004TopologicalSystems}. The emerging picture suggests that identification of the qualitative behaviour -- the bifurcation diagram -- should precede any attempt at inferring other properties of a system \cite{Stumpf2019ParameterBifurcations}.

Inferring the parameters of a model directly from a bifurcation diagram is difficult because it is not obvious how multiple  parameters in concert control the existence and position of a bifurcation. It could even be impossible for the model to bifurcate in the manner desired. Several approaches exist to place bifurcations to desired locations once a manifold is present \cite{Iwasaki1997AnType,Lu2006InverseSystems,Dobson2004DistanceBifurcations} yet typically resort to sampling techniques to search for them in the first place \cite{Chickarmane2005BifurcationTool,Conrad2006BifurcationClock}. Progress has been made in cases where model structure and stability conditions are used to refine the search space \cite{Otero-Muras2018Optimization-basedModels,Otero-Muras2014ACurves} yet the resulting objectives are still not explicit in the bifurcation targets and also not differentiable. It is always possible to bias goodness of fit during optimisation with domain-specific knowledge \cite{}, however these approaches do not aim to generalise across a wider set of qualitative behaviours. In the emerging field of scientific machine learning \cite{Rackauckas2017Differentialequations.jlJulia,Rackauckas2018ASolutions,Rackauckas2020UniversalLearning}, parameters of structured mechanistic models are favoured over flexible models in larger parameter spaces. A scalable method for navigating the space of bifurcation diagrams would enable design of differential equations with high-level qualitative constraints. Furthermore one could begin organising models according to qualitatively distinct behaviours. 

Back-propagation through differential equation solvers has been a breakthrough over the past couple of years \cite{Chen2018NeuralEquations,Rackauckas2019DiffEqFlux.jlEquations} that enabled scalable parameter inference for differential equations from trajectory data. Although one could use trajectory data to create the aforementioned qualitative constraints \cite{Ranciati2017BayesianParameters,Khadivar2021LearningBifurcations} this would entail over-constraining information originating from the kinetics and dynamical transients of the model. Furthermore, such data usually does not contain sufficient information about dynamical transients in order to identify kinetic parameters. Techniques for back-propagating through implicit equation solvers have also been developed \cite{Look2020DifferentiableLayers,Bai2019DeepModels} although to the best of the authors' knowledge have not been applied to bifurcation diagrams at the time of writing this paper.

The problem of inferring differential equation parameters against a user-specified bifurcation diagram decomposes into two parts: searching for bifurcating regimes and matching the locations of bifurcation points to desired values. Matching bifurcation locations is a supervised problem where the data are expressed as bifurcations points \cite{Lu2006InverseSystems,Conrad2006BifurcationClock}. Searching for bifurcations is an unsupervised problem because when bifurcations are not present, there is no distance defined between data and prediction \cite{Chickarmane2005BifurcationTool}. Therefore only properties of the model can be used to start the search. We propose an approach for performing both tasks in an end-to-end fashion. The bifurcation diagram encodes high-level qualitative information defined by state space structures, rather than kinetics. We apply the strategy of implicit layers \cite{Look2020DifferentiableLayers,Bai2019DeepModels} to calculate gradients. To compute the diagram we use a predictor-corrector method called deflated continuation \cite{Farrell2016TheDiagrams,Veltz2020BifurcationKit.jl} developed for partial differential equations.

We find that the cost function landscape contains basins that not only allow us to synthesise models with a desired bifurcation diagram but also allow us to organise models in terms of topological and geometric equivalence. We discuss the relevance of this in model selection. In summary, our paper has the following main contributions:

\begin{itemize}
    \item An end-to-end differentiable method for locating bifurcations in parameter space and then matching their dependency on a control condition to user-specified locations
    \item Implementation of the method as a Julia package\\
\href{https://github.com/gszep/BifurcationInference.jl}{\texttt{github.com/gszep/BifurcationInference.jl}}
    \item Leveraging the cost landscape for a novel way of organising differential equation models in terms of geometric and topological equivalence
\end{itemize}

\subsection{Preliminaries}

Suppose we collected observations along a scalar control condition $p\in\Reals$ and conclude that there are specific values of $p$ for which there are qualitative changes in system behaviour. Let $\targets$ be the set of those values and let us hypothesise that these transitions occur due to bifurcations in the dynamics that drive the underlying mechanism. Let us model the mechanism with a parameterised set of differential equations for states $u\in\Reals^N$ with a vector function $\rates$ in a parameter space $\theta\in\Reals^M$.

For the purposes of introducing this work, we will consider the simplest class of bifurcations known as \textit{co-dimension one} bifurcations not including limit cycles. Therefore $\targets$ should contain conditions for which we hypothesise changes in multi-stable behaviour. Let the equations be
\begin{align}
	\frac{\partial u}{\partial t}=\rates(u,p)
	\qquad\mathrm{where}\quad
	\rates : \Reals^{N+1}\rightarrow\Reals^N
	\label{eq:model}
\end{align}
In the context of the differential equations, and not considering limit cycles for now, we show that a static non-degenerate bifurcation can be defined by a set of conditions on the determinant of the Jacobian $\Det$. The determinant of the Jacobian quantifies the rate at which trajectories in a local patch of state-space $u\in\Reals^N$ converge or diverge. Let $s\in\Reals$ parametrise the curves that trace out the bifurcation diagram. Any location on the curve $u(s)$ and $p(s)$ must satisfy the steady-state of equations \eqref{eq:model}. The determinant approaching zero along the diagram means that the dynamics of the system are slowing down, which is an important indicator for the onset of a transition between qualitative behaviours. Furthermore, the slowing down must necessarily be followed by a breakdown of stability; for this to be true it is sufficient \textit{but not necessary} to require that the determinant cross zero with a finite slope, meaning that its directional derivative along the diagram $\frac{d}{ds}\Det$ is not zero. This is the non-degeneracy condition. The set of predicted values for the control condition $\predictions(\theta)\subset\Reals$ at which bifurcations occur are defined as
\begin{align}
	\predictions(\theta):=\left\{\,
	p\,\,\exists\,\,u:\,\,\rates(u,p)=0,\,\,\Det=0,
	\,\, \frac{d}{ds}\Det\neq0
	\,\right\}
	\label{eq:predictions}
\end{align}
A proof of how the conditions \eqref{eq:predictions} are necessary and sufficient for static non-degenerate bifurcations is detailed in Appendix \ref{appendix:conditions}. The most common bifurcations between steady states, not including limit cycles, are saddle-nodes and pitchforks \cite{Haragus2011LocalSystems}. Saddle-node bifurcations, which often appear in pairs (Figure \ref{fig:saddle-node}) are defined by stable and unstable fixed points meeting and disappearing. Pitchfork bifurcations occur where a single steady state splits into two stable and one unstable steady state (Figure \ref{fig:pitchfork} shows an \textit{imperfect} pitchfork; a \textit{perfect} pitchfork arises when $\theta_1=0$). To illustrate these bifurcations, we define minimal models (Figure \ref{fig:minimal-models}) that span the space of saddle-node and pitchforks, where indeed zero crossings in the determinant with a finite slope define the set of prediction $\predictions(\theta)$. The location of these crossings in general may not match the targets $\targets$.

\begin{figure}[ht]
\centering
\setlength\unitlength{1cm}
{\phantomsubcaption\label{fig:saddle-node}}
{\phantomsubcaption\label{fig:pitchfork}}
\includegraphics[width=5.5cm]{saddle-node}
\begin{picture}(0,0)\put(-5.5,9){\subref{fig:saddle-node}} \end{picture}
\includegraphics[width=5.5cm]{pitchfork}
\begin{picture}(0,0)\put(-5.5,9){\subref{fig:pitchfork}} \end{picture}
\caption{Illustration of bifurcation diagrams for minimal models of bifurcations. \subref{fig:saddle-node}. Saddle-node bifurcations arise for $\rates(u,p) = p + \theta_{1}u+\theta_{2}u^3$ when $\theta = (\frac{5}{2},-1)$. \subref{fig:pitchfork}. Pitchfork bifurcations arise for $\rates(u,p) = \theta_{1} + p u+\theta_{2}u^3$ when $\theta=(\frac{1}{2},-1)$. Targets are illustrated by light yellow vertical lines. Bifurcation curves are shown as solid blue and red lines, with lighter shades indicating the determinant crossing zero at locations $\predictions(\theta)$ giving rise to unstable solutions.}
\label{fig:minimal-models}
\end{figure}

For a given set of parameters $\theta$ one could compute the set of predicted bifurcations $\predictions(\theta)$ using parameter continuation methods \cite{Veltz2020BifurcationKit.jl,Farrell2016TheDiagrams}. Our goal is to find optimal parameters $\theta^*$ that match predictions $\predictions(\theta^*)$ to specified targets $\targets$. We must design a suitable cost function $\loss$ so that
\begin{align}
    \theta^* := \mathrm{argmin}_{\theta} \loss(\theta|\targets)
    \label{eq:optimal-theta}
\end{align}
The optimal $\theta^*$ is not expected to always be unique, but is in general a manifold representing the space of qualitatively equivalent models. Ideally, the cost function $\loss$ should reward $\theta$ for which the number of predicted bifurcations is equal to the number of targets, $|\predictions(\theta)|=|\targets|$. This is especially important in the case where there are no predictions $|\predictions(\theta)|=0$.

\section{Proposed Method}
\label{section:method}
\subsection{Cost Function}

To identify parameter sets that give rise to bifurcation diagrams with specified bifurcation points, we propose a cost function that comprises two terms. The role of the error term is simply to reward predicted bifurcations to coincide with the specified target locations. This of course relies on such bifurcations existing. The role of the eigenvalue term is to encourage an optimiser to move towards parameter regimes that do exhibit bifurcations.

\subsubsection{Error term: matching bifurcations to target locations}

In order for predicted bifurcations $p(\theta)\in\predictions(\theta)$ to match targets $p'\in\targets$ we need to evaluate an error term $|p(\theta)-p'|$. A naive approach might take an average over the norms for all prediction-target pairs. However this gives rise to unwanted cross-terms and the possibility of multiple predictions matching the same target without any penalty for unmatched targets. Therefore, we choose a geometric mean over the predictions and an arithmetic mean over targets:
\begin{equation}
    \error(\theta, \targets) = \frac{1}{|\targets|}\sum_{p'\in\targets}
    \prod_{p(\theta)\in\predictions(\theta)}|p(\theta)-p'|
    ^{\frac{1}{|\predictions|}}
    \label{eq:error}
\end{equation}
The error term is only zero when each target is matched by at least one prediction and allows for cases where the number of predictions is greater than or equal to the number of targets $|\predictions| \geq |\targets|$. An alternative approach, which undesirably introduces more hyper-parameters, would be to let each prediction $\predictions(\theta)$ represent the centroid of a mixture distribution and use expectation-maximisation to match the centroids to targets $\targets$.

\subsubsection{Eigenvalue term: encouraging bifurcations}
\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{bifurcation-measure}
    \caption{Bifurcation measure $\measure(s)$ and determinant $\Det$ along the arclength $s$ of two different bifurcation curves demonstrating how maximising the measure along the curve maintains the existing bifurcation marked by a circle, while encouraging new bifurcations marked by stars.}
    \label{fig:measure}
\end{wrapfigure}

We can see from Figure \ref{fig:minimal-models} and definition \eqref{eq:predictions} that predictions $p(\theta)$ can be identified by looking for points along the curve where the determinant crosses zero $\Det=0$ with a finite slope $\frac{d}{ds}\Det\neq 0$. Using these quantities we can define a positive semi-definite measure $\measure(s)$ of zero crossings in the determinant along a curve parameterised by $s$ which we define as 
\begin{equation}
    \measure(s):=
    \left(1+\left|\frac{\Det}{\frac{d}{ds}\Det}\right|\right)^{-1}
    \label{eq:measure}
\end{equation}

The bifurcation measure $\measure(s)$ is maximal at bifurcations and has finite gradients in non-bifurcating regimes (Figure \ref{fig:measure}). More specifically, the measure $\measure(s)$ is one at bifurcation points and goes to zero an odd number of times between bifurcations. This is because $\Det$ must eventually turn around in order to return back to zero, resulting in the directional derivative $\frac{d}{ds}\Det$ going to zero. Hence the measure $\measure(s)$ goes to zero for each turning point (see Figure \ref{fig:measure}).

On the other hand, as the determinant $\Det$ diverges, we approach regimes far away from any bifurcations and hence $\measure(s)\rightarrow0$. Since we would still like to have non-zero gradients with respect to $\theta$ in these regimes we designed the measure to go to zero sufficiently slowly.
\clearpage

While the calculation of the determinant is straightforward, its directional derivative requires a tangent vector to the bifurcation curve. Fortunately the tangent vector $\tangent(s)$ at the solution $u(s),p(s)$ anywhere along the curve $s$ can be calculated as the nullspace of the rectangular $N\times(N+1)$ Jacobian
\begin{equation}
    \left.
    \frac{\partial\rates}{\partial(u,p)}
    \right|_{\rates(u(s),p(s))=0}
    \cdot\tangent(s)=0
    \label{eq:tangent}
\end{equation}
This equation guarantees that the tangent vector $\tangent(s)$ is orthogonal to all hyper-planes defined by the components of $\rates$. In this setting the dimension of the nullspace is always known, and therefore can reliably be calculated using QR factorisation methods \cite{Drmac2008OnStudy}.

Equipped with a measure that quantifies the appearance of bifurcations along a bifurcation arc we can define the total measure for a bifurcation diagram as
\begin{equation}
    \Psi(\theta):=\frac{
        \int_{\rates(u,p)=0}\!
        \measure(s)\,\mathrm{d}s
    }{
        \int_{\rates(u,p)=0}\!\
        \mathrm{d}s
    }.
    \label{eq:total-measure}
\end{equation}
Here we denote $\int_{\rates(u,p)=0}\mathrm{d}s$ as the sum of the line integrals in $(u,p)\in\Reals^{N+1}$ defined by the level set $\rates(u,p)=0$ with $s$ being an arbitrary parametrisation of the curves. The total measure $\Psi(\theta)$ is normalised such that $\Psi(\theta)\rightarrow1$ in the regimes where the controlled condition region $p$ is densely packed with bifurcations. The total measure $\Psi(\theta)$ is added to the error term as if it were a likelihood. This defines the cost function as
\begin{align}
    \loss(\theta|\targets):=
    \big(|\predictions|-|\targets|\big)\log\Psi(\theta)+
    \error(\theta, \targets),
    \label{eq:loss}
\end{align}
The pre-factor $|\targets|-|\predictions|$ in the eigenvalue term ensures that the gradients are always pushing optimisers towards a state where $|\targets|=|\predictions|$. This can be seen as a step-wise annealing of the eigenvalue term until the desired state is reached.

\subsection{Differentiating the cost function}

To make use of gradient-based optimisers to locate desired bifurcation diagrams, we show here how to differentiate the cost function. First, we note that while individual bifurcations $p(\theta)$ depend smoothly on $\theta$, the total number of predictions $|\predictions|$ does not have gradient contributions with respect to $\theta$. Therefore, we can safely drop the dependency in the prediction counter and now proceed in taking gradients with respect to $\theta$ knowing that the only dependencies we need to track are for individual bifurcations $p(\theta)$ within the definition the error term \eqref{eq:error} and the total measure \eqref{eq:total-measure}. Therefore, 
\begin{align}
    \frac{\partial\loss}{\partial\theta}=
    \big(|\predictions|-|\targets|\big)\,\lambda\,
    \frac{\partial \Psi}{\partial\theta}\Psi(\theta)^{-1}+
    \frac{1}{|\targets||\predictions|}\sum_{p'}
    \prod_{p(\theta)}|p(\theta)-p'|^{\frac{1}{|\predictions|}}
    \sum_{p(\theta)}\frac{\partial p}{\partial\theta}\left(p(\theta)-p'\right)^{-1}
    \label{eq:gradient-loss}
\end{align}
In a similar vein to back-propagation through neural differential equations \cite{Chen2018NeuralEquations} we would like to be able to calculate the gradient $\frac{\partial\loss}{\partial\theta}$ without having to differentiate through the operations of the solver that finds the bifurcation diagram $\rates(u,p)=0$ and the bifurcation locations $p(\theta)$. To calculate the gradient of the measure $\frac{\partial \Psi}{\partial\theta}$ we need to differentiate line integrals that depend on $\theta$. Fortunately this can be done by the application of the generalised Leibniz integral rule, details of which can be found in Appendix \ref{appendix:leibniz-rule}.

The gradient of the bifurcation points $\frac{\partial p}{\partial\theta}$ is found by application of the implicit function theorem to a vector function $G_{\theta}:\Reals^{N+1}\rightarrow\Reals^{N+1}$ whose components represent the two constraints $\rates(u,p)=0$ and $\Det=0$. By following a similar strategy to that used by implicit layers \cite{Look2020DifferentiableLayers} we yield an $(N+1)\times M$ Jacobian representing a deformation field \cite{Jos2011OnSurface} for each $\theta$ direction. The gradient we are looking for becomes
\begin{align}
    \frac{\partial p}{\partial\theta} = -\hat{p}\cdot\left.\frac{\partial G_{\theta}}{\partial (u,p)}^{-1}
    \frac{\partial  G_{\theta}}{\partial\theta}\right|_{G_{\theta}(u,p)=0}
    \quad\mathrm{where}\quad
    G_{\theta}(u,p):=\begin{bmatrix}\rates(u,p)\\\Det\end{bmatrix}
    \label{eq:gradient-bifurcation}
\end{align}
Here $\hat{p}$ is a unit vector in $(u,p)\in\Reals^{N+1}$ that picks out the deformations along the $p$-direction. If we wanted to place the bifurcation at target steady state $u'$ as well as target control condition $p'$ we would use the full $(N+1)\times M$ deformation matrix. Calculation of this matrix involves inverting an $(N+1)\times(N+1)$ Jacobian $\frac{\partial G_{\theta}}{\partial(u,p)}$. Instead of explicitly inverting the Jacobian the corresponding system of linear equations is solved. The determinant of this Jacobian goes to zero in the degenerate case where $\frac{d}{ds}\Det=0$, further justifying our choice of measure $\Psi(\theta)$ which discourages the degenerate case.

The cost function is piece-wise smooth and differentiable with undefined gradients only in parameter contours where the number of predictions $|\predictions|$ changes; this is when $\Psi(\theta)$ is undefined and the inverse of $\frac{\partial G_{\theta}}{\partial (u,p)}$ does not exist. Given a set of solutions to $\rates(u,p)=0$ and locations $p(\theta)$ the gradient $\frac{\partial\loss}{\partial\theta}$ can be evaluated using automatic differentiation methods \cite{Revels2016Forward-ModeJulia,Innes2018FashionableFlux,Innes2018Flux:Julia} without needing to back-propagate through the solver that obtained the level set $\rates(u,p)=0$ in the forward pass.

\section{Experiments \& Results}
\label{section:results}
In this section, we apply the method first to minimal examples that can produce saddle-node and pitchfork bifurcations, and then a more complex model that has multiple parametric regimes that produce saddle-node bifurcations.

\subsection{Minimal Models}
Optimisations of two parameters $(\theta_1,\theta_2)$ using simple gradient descent from \texttt{Flux.jl} with learning rate $\eta=0.01$ for the minimal saddle-node and pitchfork models yield trajectories approaching lines of global minima in the cost function (Figures \ref{fig:minimal-models:results}) which represent a set of geometrically equivalent models. Two bifurcation diagrams are geometrically equivalent if the number, type and locations of bifurcations match the specified targets $\targets$.

\begin{figure}[ht]
\centering
\includegraphics[width=6cm]{saddle-landscape.png}
\includegraphics[width=6cm]{saddle-optima.png}
\includegraphics[width=6cm]{pitchfork-landscape.png}
\includegraphics[width=6cm]{pitchfork-optima.png}
\caption{Saddle-node $\rates(u,p) = p + \theta_{1}u+\theta_{2}u^3$ and pitchfork $\rates(u,p) = \theta_{1} + u p +\theta_{2}u^3$ optimised with respect to $\theta$ so that predicted bifurcations $\predictions(\theta)$ match targets $\targets$ in control condition $p$. The right panel shows bifurcations diagrams for the three optimal $\theta^*$ marked by stars on the left panel. The optimisation trajectories in white follow the gradient of the cost, approaching the black lines of global minima in the left panel}
\label{fig:minimal-models:results}
\end{figure}

We can see that the geometrically equivalent lines are contained within larger basins where the correct number and type of bifurcations are present but do not match the locations of targets $\targets$. All models within this basin are in some sense topologically equivalent. This hierarchical classification allows us to identify the set of models that satisfy observed qualitative behaviour \cite{Stumpf2019ParameterBifurcations} before any attempt at inferring kinetic parameters, which is done by choosing a model along the line of geometrically equivalent models.

Optimisation trajectories for the two minimal models appear mostly circumferential. This is because the models were set up such that the radial direction from the origin in $\theta$ space mostly scale kinetics whereas the circumferential direction changes the bifurcation topology. This suggests that the gradients of our cost function seek to change model geometry over kinetics.

\subsection{Genetic Toggle Switch}
In this section we optimise a model where the states share a Hill function relationship with cooperatively $n=2$; these models often emerge from mass action kinetics with quasi-steady state approximations and are used to model species concentrations. After re-scaling the equations governing the dynamics of concentrations, the simplified equations for state $u_1$ and $u_2$ become 
\begin{equation}
    \partial_t u_1 = \frac{ a_1 + (p u_2)^2}{ 1 + (p u_2)^2 } - \mu_1 u_1 \quad
    \partial_t u_2 = \frac{ a_2 + (k u_1)^2}{ 1 + (k u_1)^2 } - \mu_2 u_2
    \label{eq:two-state}
\end{equation}
where $a_k$ is the baseline production rate for species $k$ in the absence of the other species. Each species has a finite degradation rate $\mu_k$. Finally we have two sensitivity constants $p$ and $k$, one of which is chosen as our control condition. A baseline production rate $a_k>1$ recovers an inhibitor type hill function for species $k$ and is an activator otherwise. The sensitivities are proportional to the slope of the hill productions. Solving for the steady states,  substituting the equation for $u_1$ into $u_2$ and rearranging gives rise to the relationship
\begin{equation}
    \dfrac{k}{\mu_1} = \dfrac{(1 + (\frac{p}{\mu_2} u')^2) \sqrt{a_2 - u'} }{ (a_1 + (\frac{p}{\mu_2} u')^2) \sqrt{ u' - 1 } }
    \quad\mathrm{where}\quad u':= u_2\mu_2
    \label{eq:steady-states}
\end{equation}
which reveals that only $a_1$, $a_2$ and the ratio between the sensitivity and degradation parameters, $\frac{k}{\mu_1}$, affect the solutions to this equation, and hence the locations of the bifurcations (Figure \ref{fig:two-state-optima:parameters}). In 98\% of 800 runs, optimisation using the ADAM optimiser \cite{Kingma2014} from \texttt{Flux.jl} with learning rate $\eta=0.1$ converged to one of two clusters: mutual activation ($a_1 < 1, a_2 < 1$; cluster 1) and mutual inhibition ($a_1 > 1, a_2 > 1$; cluster 2) regimes. Example bifurcation diagrams illustrate how the bifurcation curves of each species are positively correlated in mutual activation and negatively correlated for mutual inhibition (Figure \ref{fig:two-state-optima:models}).

\begin{figure}[ht]
\centering
\setlength\unitlength{1cm}
{\phantomsubcaption\label{fig:two-state-optima:parameters}}
{\phantomsubcaption\label{fig:two-state-optima:models}}
\includegraphics[width=9cm]{two-state-optima}
\begin{picture}(0,0) \put(-9,6.3){\subref{fig:two-state-optima:parameters}} \end{picture}
\begin{picture}(0,0) \put(-3,6.3){\subref{fig:two-state-optima:models}}
\end{picture}
\caption{Bifurcation inference for the two-state model \eqref{eq:two-state}. \subref{fig:two-state-optima:parameters}. Optimal parameter estimates $\theta^*$ for the targets $\targets=\{4,5\}$ reveal two clusters of qualitatively different regimes: mutual activation ($a_1 < 1$; cluster 1) and mutual inhibition ($a_1 > 1$; cluster 2). \subref{fig:two-state-optima:models}. Example bifurcation diagrams indicate positively and negatively correlated dependencies between the two model states, as a function of the control condition.}
\label{fig:two-state-optima}
\end{figure}

In order to maintain biological interpretability, optimisation was restricted to the positive parameter regime by transforming the parameters to log-space $\theta\rightarrow10^\theta$. At the beginning of each optimisation run an initial $\theta$ was chosen in the log-space by sampling from a multivariate normal distribution with mean zero and standard deviation one.


%\clearpage
\subsection{Complexity}

Our method can be used for more complex models than the minimal models shown in Figure \ref{fig:minimal-models:results}. To demonstrate this we applied the model to a 4 state 40 parameter model \cite{Grant2020InterpretationCircuit} (see Appendix \ref{appendix:double-exclusive}).

Performing one iteration of the optimisation requires the computation of the gradient of the cost \eqref{eq:gradient-loss}, requiring a computation of the bifurcation diagram with parameter continuation methods, which includes the evaluation of matrix inversions \eqref{eq:gradient-bifurcation}. Instead of evaluating the inversions directly, we solve a system of linear equations, applying the same strategy as implicit layers \cite{Look2020DifferentiableLayers,Bai2019DeepModels}. This leaves us with the computational bottleneck of calculating the determinant of the state space Jacobian, required in both the bifurcation measure \eqref{eq:measure} and gradient \eqref{eq:gradient-bifurcation}. This calculation scales like $N^2$ where $N$ is the number of state space variables (Figure \ref{fig:scaling}).

\begin{figure}[ht]%{r}{0.5\textwidth}
\centering
\includegraphics[width=0.48\textwidth]{scaling}
\caption{Complexity scaling of calculating the gradient of the cost function. Calculations were performed on an Intel Core i7-6700HQ CPU @ 2.60GHz x 8 without GPU acceleration}
\label{fig:scaling}
\end{figure}

For this calculation a model \eqref{eq:scaling} was chosen that so that it is extensible both in the number of parameters $M$ and the number of states $N$. 
\begin{equation}
\begin{cases}
    \,\,\partial_t u_1 = \sin^{2}\!p - (\theta_1\sin^{2}\!p+1)u_1\\
    \,\,\partial_t u_n = u_{n-1} - (\mu_n^2+1) u_n & 2\le n\leq N
\end{cases}
\label{eq:scaling}
\end{equation}
In this model only the first state $u_1$ defines the shape of the bifurcation diagram, while the remaining states are merely linearly proportional to the first. The parameters $\mu_n$ contain sums of $\theta_m$ allowing us a flexible choice on the number of parameters while maintaining stable solutions for the bifurcation diagram.

While still tractable on laptop computers for states $N<100$ our implementation currently does not scale well for partial differential equations where a large the number of states $N$ arises from discretisation of the spatial variables. The only reason we need this determinant is because it is an indicator of bifurcations. We can address the computational bottleneck by finding a more computationally efficient way of calculating this indicator. One approach would be to take the product of a finite subset of eigenvalues of the system. Note that any more efficient calculation must still permit back-propagation through it.

\section{Conclusion \& Broader Impact}
\label{section:conclusion}

We proposed a gradient-based approach for inferring the parameters of differential equations that produce a user-specified bifurcation diagram. By applying implicit layers \cite{Look2020DifferentiableLayers,Bai2019DeepModels} and the generalised Leibniz rule \cite{Flanders1973DifferentiationSign} to the geometry of the implicitly defined steady states \cite{Goldman2005CurvatureSurfaces} it is possible to use automatic differentiation methods to efficiently calculate gradients. We defined a bifurcation measure that uses the determinant of the state-space Jacobian as an indicator for bifurcating parameter regimes in the eigenvalue term of the cost function. The gradients of the cost can be efficiently computed using automatic differentiation methods. The computational bottleneck is the evaluation of the state-space Jacobian determinant which limits the implementation to ordinary differential equations.

We demonstrated our approach on models with one bifurcation parameter that can give rise to pitchforks and saddle-nodes. The estimated parameters form distinct clusters, allowing us to organise models in terms of topological and geometric equivalence. In the case of the genetic toggle switch we recovered mutual activation and inhibition regimes.

Although we did not consider limit cycles, the bifurcation measure can be extended to detect Poincar\'e-Andronov-Hopf bifurcations alongside changes in stability of fixed points (see Appendix \ref{appendix:hopf} for details).This measure enables detection of the onset of damped oscillations and/or the emergence of limit cycles (Figure \ref{fig:hopf-measure}). Used together with a steady state solver that detects periodic solutions and gradient-based optimisation, we can specify regions of damped oscillation and limit cycles. Our approach generalises naturally to bifurcation manifolds such as limit point curves or surfaces. This is because the normal components of implicit derivatives can still be calculated for under-determined systems of equations \cite{Jos2011OnSurface,Tao2016Near-IsometricTracking,Fujisawa2013CalculationInvariance}. In the case of manifolds it would be more appropriate to use isosurface extraction algorithms rather than continuation to obtain the steady-state manifold (further discussion in Appendix \ref{appendix:stedy-state-solvers}). Our approach does not depend on the details of the steady-state solver and therefore can still be applied.

In dynamical systems theory the geometry of state-space determines all of the qualitative behaviours of a system. Our work makes progress towards designing models directly in state-space, rather than the spatial or temporal domain. This is valuable to experimentalists who only have qualitative observations available to them and wish to navigate the space of qualitative behaviours of their system. Our work lies within a trend of progress in the scientific machine learning community, where structured domain-informed models are favoured over flexible models that live in large parameter spaces.

\section{Acknowledgements}
We would like to acknowledge Kieran Cooney for the fruitful conversations that helped guide the derivations and computational approach. A special thanks go to Romain Veltz and the Julia community for helpful pointers on package development and discussions over Slack.

\bibliography{refs}
\bibliographystyle{ieeetr}

\clearpage\pagenumbering{arabic}\setcounter{page}{1}
\section*{Appendix}
\appendix
\input{appendix}

\end{document}